## Атомарность

Довольно пугающий термин, если столкнуться с ним впервые. Но, в целом, сам по себе он достаточно прост в объяснении, если у вас есть некоторая база

### TL;DR

Постараюсь ёмко изложить материал про атомарность, атомики и как это работает, зачем и почему.

## Изобретаем атомарность

### Прикольный пример

Грубо говоря, атомарность нужна нам, если у нас существует более одного потока исполнения. То есть, если вдруг один поток может прерваться и начать исполняться другой. А если они будут исполняться параллельно, то тогда уж тем более потребуется синхронизировать данные. В целом, самый тупой вариант — мьютекс. Но мы ведь ходим понять что такое атомарность?

Рассмотрим следующий код.

[link](https://godbolt.org/z/qWzGreMac): 
```cpp
int a{0};

std::thread t1 {
    [&]() {
        for (size_t i = 0; i < N; i++)
            a++;
    }};


std::thread t2 {
    [&]() {
        for (size_t i = 0; i < N; i++)
            a--;
    }};

t1.join(); // дожидаемся исполнения первого потока
t2.join(); // дожидаемся исполнения второго потока
```

Вопрос: а какое значение `a` будет иметь после исполнения этого кода?

Правильный ответ: никакое. Ну, то есть, оно конечно будет, но вообще в рамках C++ это UB. Однако на практике же мы получим какое-то случайное число, если `N` будет достаточно велико.
Вот несколько значений, которые выводит программа, если запустить её в цикле
```
10941
10637
1246
-39806
100000
50751
-32123
-34587
27949
```
### Небольшое введение в работу компьютера

Наш процессор не работает напрямую с данными из RAM, а собирает их в кэш. Это некое место в самом процессоре, которое очень очень быстро позволяет писать данные в него, в отличии от RAM, поскольку оперативная память является как бы внешним устройством.
Упрощённо есть 3 типа кэшей:

- L1 — Кэш каждого ядра, размер 32Kb
- L2 — Кэш для одного ядра, но более медленный чем L1, размер 256Kb
- L3 — Кэш общий для всех ядер, размер 2Mb

### Как это происходит наглядно

Ради хорошей жизни не буду писать здесь много подробностей.

```
Поток 1: Копирует себе часть общего кеша (a=100)
Поток 2: Копирует себе часть общего кеша (a=100)
Поток 2: Прибавляет переменной a единицу (a=101)
Поток 1: Прибавляет переменной a единицу (a=101)
Поток 1: Сохраняет свой кеш в общий (a=101)
Поток 2: Сохраняет свой кеш в общий (a=101)
```

Ну и как бы вот за счёт этого эффекта мы теряем некоторые значения.

### Как решается эта проблема
Когерентность кэша (англ. cache coherence) — свойство кэшей, означающее целостность данных, хранящихся в локальных кэшах для разделяемого ресурса. Когерентность кэшей — частный случай когерентности памяти.
(Взято из [wiki](https://ru.wikipedia.org/wiki/%D0%9A%D0%BE%D0%B3%D0%B5%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D0%BE%D1%81%D1%82%D1%8C_%D0%BA%D1%8D%D1%88%D0%B0)). 

Есть множество протоколов, например MESI. 
AMD, например, использует MOESI, а Intel использует MESIF.

Кеш это вот много данных (пару килобайт, например), а кеш-линия это обычно что-то измеряемое байтами, 64 или 128, например.

В чём же заключается суть этого протокола? 
У каждой кеш линии есть одно из четырёх состояний:

- Modified — состояние изменено только в этом кэше, данные отличаются от данных в RAM
- Exclusive — состояние есть только в этом кэше и совпадает с RAM
- Shared — состояние есть и в других кэшах и совпадает с RAM
- Invalid — невалидное состояние кэш-линии

В итоге, у нас есть некоторые операции, которые позволяют инвалидровать кэш-линии. Работают они примерно так:
```
Поток 1: Загружает кэш-линию
    a = 100
    Кеш линия 1 = E (Значение есть только в этой кеш-линии)

Поток 2: Загружает кэш-линию
    a = 100
    Кеш линия 1 = S (Значение есть в нескольких кеш-линиях и совпадает)
    Кеш линия 2 = S

Поток 2: Атомарно прибавляет переменной a единицу.
    a = 101
    Кеш линия 1 = I (пред. S) (Состояние в этой кеш-линии устарело прямо в момент записи. Произошёл так называемый Request For Ownership)
    Кеш линия 2 = E (пред. S) (Состояние только в этой кеш-линии)

Поток 1: Видит, что кэш-линия невалидная и идёт загружать актуальную версию.
    Кеш линия 1 = S (пред. I) (Значение есть в нескольких кеш-линиях и совпадает)
    Кеш линия 2 = S (пред. E) (Значение есть в нескольких кеш-линиях и совпадает)

Поток 1: Прибавляет переменной a единицу
    a = 102
    Кеш линия = M (пред. I) 
    Кеш линия 1 = E (пред. S) (Состояние только в этой кеш-линии)
    Кеш линия 2 = I (пред. S) (Состояние в этой кеш-линии устарело)
```

Если вы ничего не поняли — плохо. Но я бы не стал отчаиваться и попробовал бы ещё раз.

В случае, когда у нас не было атомарности, то мы не инвалидировали кэш-линию, за счёт чего происходили страшные вещи. При этом, даже это объяснение несколько некорретно и неполно, но думаю, что смысла его дополнять ещё больше нет, если необходимо поверхностное погружение в тему.

### Давайте уже писать код

***
**Delete this**

Если честно, я уже устал писать и читать огромные полотна текста и хочу просто уже написать хоть что-то работающее! 
***


Собственно. В `C++` есть решение этой проблемы в виде `std::atomic<T>`, которое предоставляет интерфейс для атомарных операций.

[link](https://godbolt.org/z/Taf3h1jWz).
```cpp
std::atomic<int> a{0};

std::thread t1 {
    [&]() {
        for (size_t i = 0; i < N; i++)
            a++;
    }};


std::thread t2 {
    [&]() {
        for (size_t i = 0; i < N; i++)
            a--;
    }};

t1.join();
t2.join();
```

В данном примере мы будем всегда получать `a==0`, потому что все операции атомарны. То есть, не будет прокола с кешами.

### А что поменялось в ассемблере?

По сути для AMD64, с некоторыми допущениями, инструкция `add` была использована с префиксом `lock`. По существу хотелось бы ещё добавить, что мы в первом случае будем писать в регистр, который потом куда-то засунем, а во втором мы пишем сразу в какую-то область памяти.
```asm
add         edx, 1          ; до атомика
lock add    DWORD PTR [rdx], 1 ; после атомика
```

## Атомики

### Атомарность



### Доступные операции
В `C++` класс `std::atomic<T>` имеет следующие методы:

- Store — Загрузить значение в переменную
- Load — Прочитать значение из переменной
- Exchange — Записать новое значение и получить старое
- CAS — Сравнивает текущее значение с ожидаемым и, если они равны, то записывает желаемое значение, а если не равны, то записывает старое в желаемое, а после возвращает равны они или нет (да-да, это сложно, однако на примере куда яснее)

Со всеми операциями вроде понятно, но что `compare_exchange` выглядит страшно. При этом в `C++` их целых два: `std::atomic<T>::compare_exchange_weak, std::atomic<T>::compare_exchange_strong`. Если очень грубо говоря, то `strong` функция возвращает `false` тольо если гарантированно значения различны, а `weak` же может вернуть `false` для одинаковых значений. Называется это `fail spuriously` (неожиданно провалиться). 

Я бы рекомендовал использовать `weak` для спинлоков (`CAS` в цикле) и `strong` в остальных случаях, но, строго говоря, никто не мешает вам делать иначе.

Собственно, рассмотрим на примере, как именно работает `CAS` (За форматирование не бейте, я старался делать строки более короткими):

[link](https://godbolt.org/z/1q3479Eos).
```cpp
std::atomic<int> a = 14;

int x = 14, y = 42;
auto result1 = a.compare_exchange_strong(x, y);

std::cout
    << "expected = " << x
    << "; desired = " << y
    << "; result = " << result1
    << "; atomic = " << a.load()
    << std::endl;

int z = 10, w = 32;

auto result2 = a.compare_exchange_strong(z, w);

std::cout
    << "expected = " << z
    << "; desired = " << w
    << "; result = " << result2
    << "; atomic = " << a.load()
    << std::endl;
```

При запуске кода мы увидим следующий вывод:
```
expected = 14; desired = 42; result = 1; atomic = 42
expected = 42; desired = 32; result = 0; atomic = 42
```

Что он означает? Прочитаем первую строку: изначально результат удался и в `expected` нам записали старое значение атомика равное `14`.
Вторая строчка же говорит о том, что `CAS` не удалось выполнить. Это случилось из-за того, что `expected == 10` не было равно `42`. Поэтому в `expected` записали актуальное значение атомика и вернули `false`, так-как операция не удалась.

Если до сих пор не стало понятно, то рекомендую залезть и поиграться с этим. Но, вероятно, у вас возникает вопрос: а зачем это нужно? Давайте разберём более интересный пример!

### Пример

Полный пример с небольшими тестами можно посмотреть ниже по ссылке.

[link](https://godbolt.org/z/j8fMzojrM).

```cpp
class ts_stack {
private:
    struct Node {
        Node* prev;
        int value;
    };
    std::atomic<Node*> m_tail;

public:
    ts_stack() : m_tail(nullptr) {}

    bool try_add(int val) {
        // Сохраняем значение указателя на конец
        Node* old = m_tail.load();

        // Создаём новую ноду с указателем на конец
        auto node = new Node{old, val};

        // Проверяем не поменялось ли старое значение ноды
        // Если поменялось, то значит операция не удалась
        // и тогда мы удаляем нашу ноду и выходим из функции
        // мол неудачно
        auto res = m_tail.compare_exchange_strong(old, node);

        if (!res)
            delete node;

        return res;
    }

    bool try_pop(int& val) {
        // Загружаем старое значение
        auto old = m_tail.load();
        // Если оно nullptr, то мы возвращаем false
        if (old == nullptr)
            return false;
        // Теперь пытаемся заменить хвост на предпоследний элемент
        // Если он успел поменяться, то мы просто выходим из функции
        // и возвращаем при этом false, мол не смогли поменять
        auto res = m_tail.compare_exchange_strong(old, old->prev);
        if (!res)
            return false;
        // Однако, если нам удалось поменять всё таки значение,
        // то мы теперь должны его записать куда-то
        // можно было бы и возвращать, но вообще куда
        // проще передавать ссылку на значение
        val = old->value;

        // по хорошему надо бы ещё и удалять Node*,
        // потому что мы достаточно много памяти расходуется зряв
        // но об этом чуть позже
        return true;
    }
};
```

Собственно, вроде как я достаточно написал комментариев к коду, чтобы было понятно, что, почему и как сделано. Конечно, по хорошему стоило бы начать с рассмотрения якобы правильных стеков и понять, почему именно не стоит делать так, как делают там, но поскольку статья обзорная, то я оставлю это на какие-нибудь профильные книги.

А теперь, если вы всё поняли, то хочу обратить ваше внимание на момент, почему я не стал в `try_pop` делать `delete old`. 

Собственно, давайте смотреть!

```
Поток 1:
    try_pop:
        Node* old = m_tail.load();
        Потом вернёмся сюда
Поток 2:
    try_pop:
        val = old->value;
        delete old; // представим, что я добавил эту строчку
Поток 1:
    try_pop:
        возвращаемся к тому, где были
        if (old == nullptr)
            return false;
        auto res = m_tail.compare_exchange_strong(old, old->prev);
```

Собственно, что же тут случилось? А случилось вот что: после вызова `delete` для `old` мы обращаемся `old->prev`. 

Иными словами мы в двух потоках захватили один и тот же адрес хвоста, а потом в одном удалили быстрее, чем в другом закончили работу с ним. Для простоты картины я решил не решать эту проблему, но если вкратце она именуется Retention Policy и у неё есть несколько способов решения. Например, через `shared_ptr` или кастомный аллокатор.


## Хотя кажется есть нюанс

### Ищем новые проблемы
Вообщем-то, после всего пережитого ужаса нам очень хочется верить, что ужас заканчивается, но... Нет! Он только начинается. 

Пока я ко всем атомарным операциям добавлю ещё один параметр `std::memory_order::relaxed` и не буду объяснять зачем это нужно, но буду верить, что вы догадаетесь.

[link](https://godbolt.org/z/KG7reKfeG).
```cpp
std::atomic<int> x{0}, y{0};
std::atomic<bool> finish = false;

void thread1() {
    while (!finish.load(std::memory_order::relaxed)) {
        x.fetch_add(1, std::memory_order::relaxed);
        y.fetch_add(1, std::memory_order::relaxed);
    }
}

void thread2() {
    while (!finish.load(std::memory_order::relaxed)) {
        auto a = x.load(std::memory_order::relaxed);
        auto b = y.load(std::memory_order::relaxed);
        if (b > a) {
            std::cout << "x != y, " << a << " != " << b << std::endl;
            finish.store(true, std::memory_order::relaxed);
        }
    }
}

int main() {
    std::thread t1{thread1};
    std::thread t2{thread2};
    t1.join();
    t2.join();
    return 0;
}
```

И так... ваши ставки. Что произойдёт?

1. Ошибка компиляции
2. Программа будет жить вечно, ведь если операции атомарны, то как может быть `y>x` или `b>a`
3. Программа рано или поздно завершиться из-за того, что случился прикол

И, как, наверное, уже подсказывает интуиция многие люди ответили, что всё таки произойдёт вариант под номером 3. И это правильный ответ. Вернее, конечно, оно может и не произойти, но скорее всего произойдёт. 

А что происходит? А происходит переупорядочивание операций.

### MemoryOrder
Собственно, процессор в качестве оптимизаций может переставлять ваши операции местами. Почему? Ну вот умные дядьки так решили, а мы им и верим.

memory_order_consume намеренно не буду упоминать далее, потому что он deprecated, да и не видел особо вариантов использования. 

Собственно, давайте  начнём с определений
- memory_order_relaxed — атомарная операция без барьеров. То есть, операции могут быть переставлены как ДО, так и ПОСЛЕ.
- memory_order_acquire — это барьер на операции ПОСЛЕ. Никакая операция чтения/записи не может быть переупорядочена с данной, если она идёт ПОСЛЕ неё. Однако если она идёт перед, то может. (load)
- memory_order_release — это барьер на операции ДО. Никакая операция чтения/записи не может быть переупорядочена с данной, если она идёт перед ней. Однако если она идёт ПОСЛЕ, то может. (store)
- memory_order_acq_rel — это барьер на операции ПОСЛЕ и ДО. Никакая операция чтения/записи не может быть переупорядочена с данной. (load + store)
- memory_order_seq_cst — самая строгая гарантия. Она позволяет сохранять порядок абсолютно для всех потоков. Но чем же она отличается от memory_order_acq_rel — вопрос хороший.


### Немного примеров

Рассмотрим несколько примеров

1.

```cpp
int x = 0;
std::atomic<bool> flag{false};


void thread_1() {
    x = 14;
    flag.store(true, std::memory_order_relaxed);
}

void thread_2() {
    
    while (true) {
        if (flag.load(std::memory_order_relaxed)) {
            auto res = x == 14;
            // res может быть равен false
            // потому что x == 0,
            // ибо первый поток ещё не успел загрузить значение
        }
    }
}
```

2.

```cpp
void thread_1() {
    x = 14;
    flag.store(true, std::memory_order_release);
}

void thread_2() {
    
    while (true) {
        if (flag.load(std::memory_order_relaxed)) {
            auto res = x == 14;
            // res может быть равен false
            // потому что x == 0,
            // ибо этот поток загрузил значение
            // ещё до операции flag.load
        }
    }
}
```

3.

```cpp
void thread_1() {
    x = 14;
    flag.store(true, std::memory_order_release);
}

void thread_2() {
    
    while (true) {
        if (flag.load(std::memory_order_acquire)) {
            auto res = x == 14;
            // res не может быть равен false
        }
    }
}
```

Думаю, что здесь всё стало на свои места и теперь более очевиден момент с перестановкой операций. Но вообще этот код в теории содержит UB, поэтому я бы сказал, что это лишь иллюстрация, для наглядной демонстрации memory_order.

### Порядок Sequentially-consistent 

**Как будто-бы не очень понятно описал. Стоило бы дополнить немного или почитать побольше, получше материалы, чтобы найти качественный**

Имхо, лучший пример для демонстрации этого можно найти на cppreference

(предоставить ссылку позже)

```cpp
#include <atomic>
#include <cassert>
#include <thread>
 
std::atomic<bool> x = {false};
std::atomic<bool> y = {false};
std::atomic<int> z = {0};
 
void write_x()
{
    x.store(true, std::memory_order_seq_cst);
}
 
void write_y()
{
    y.store(true, std::memory_order_seq_cst);
}
 
void read_x_then_y()
{
    while (!x.load(std::memory_order_seq_cst))
        ;
    if (y.load(std::memory_order_seq_cst))
        ++z;
}
 
void read_y_then_x()
{
    while (!y.load(std::memory_order_seq_cst))
        ;
    if (x.load(std::memory_order_seq_cst))
        ++z;
}
 
int main()
{
    std::thread a(write_x);
    std::thread b(write_y);
    std::thread c(read_x_then_y);
    std::thread d(read_y_then_x);
    a.join(); b.join(); c.join(); d.join();
    assert(z.load() != 0); // will never happen
}
```

Давайте вдумаемся, что же тут происходит на уровне простых логических действий.
Запускается 4 потока:
- Поток 1: пишет в переменную `x` значение `true`
- Поток 2: пишет в переменную `y` значение `true`
- Поток 3: ждёт пока переменная `x` будет `true` пишет в переменную `z` значение на единицу больше, если `y` уже записан
- Поток 4: ждёт пока переменная `y` будет `true` пишет в переменную `z` значение на единицу больше, если `x` уже записан

Выше написан код с `seq_cst` мемори ордером, что позволяет синхронизировать значения `x` и `y` между собой. Однако давайте перепишем код на `acq/rel` memory order и посмотрим, что именно компилятор переупорядочивает, а что нет


```cpp
void write_x()
{
    // никаких операций до и после нет,
    // поэтому и нечего переупорядочивать
    x.store(true, std::memory_order_release);
}
 
void write_y()
{
    // никаких операций до и после нет,
    // поэтому и нечего переупорядочивать
    y.store(true, std::memory_order_release);
}
 
void read_x_then_y()
{   
    // все чтения не могут быть вынесены вверх
    while (!x.load(std::memory_order_acquire))
        ;
    // однако в этот момент y может быть ещё не записан, если первый поток отработал, а второй нет
    if (y.load(std::memory_order_acquire))
        ++z;
}
 
void read_y_then_x()
{
    // все чтения не могут быть вынесены вверх
    while (!y.load(std::memory_order_acquire))
        ;
    // казалось бы, если бы предыдущий поток не сработал,
    // то этот точно бы сработал...
    // но, нет.
    if (x.load(std::memory_order_acquire))
        ++z;
}
```